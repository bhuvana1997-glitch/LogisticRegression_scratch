# LogisticRegression_scratch
implementing  and interpreting logictic regression from scratch
1.Project Overview
This project demonstrates the implementation of Logistic Regression from scratch using Python and NumPy.
The dataset is synthetically generated using sklearn.datasets.make_classification, while all core components of logistic regression are manually implemented without using any machine learning model from libraries.
The objective is to understand the mathematical foundations of logistic regression and how gradient descent optimizes model parameters.
2.Objectives
Generate a synthetic binary classification dataset
Implement logistic regression without using sklearn models
Understand sigmoid activation and binary cross-entropy loss
Train the model using gradient descent
Evaluate model performance using accuracy and confusion matrix
Compare predicted labels with true labels
3.Technologies Used
Python
NumPy
scikit-learn (only for dataset generation)
4.Dataset
Generated using sklearn.datasets.make_classification
Number of samples: 200
Number of features: 2
Number of classes: 2 (binary classification)
5.Implementation Details
1. Sigmoid Function
The sigmoid function maps any real-valued input into a range between 0 and 1 and is used to model probabilities.
2. Binary Cross-Entropy Loss
The loss function measures how well the predicted probabilities match the true labels.
3. Gradient Descent
Gradient descent is used to iteratively update weights and bias to minimize the loss function.
4. Training
The model is trained over multiple epochs using forward propagation, loss calculation, gradient computation, and parameter updates.
5. Prediction
Predictions are generated by applying a threshold of 0.5 to the sigmoid output.
6.Model Evaluation
The following evaluation metrics are calculated using custom implementations:
Accuracy
Confusion Matrix (True Negatives, False Positives, False Negatives, True Positives)
A comparison table is also displayed to show true labels versus predicted labels.
7.Learning Outcomes
Clear understanding of how logistic regression works internally
Hands-on experience with gradient descent optimization
Ability to implement machine learning algorithms from first principles
8.Conclusion
This project provides a strong foundation in binary classification using logistic regression and demonstrates how core machine learning algorithms can be built from scratch using basic numerical operations.
